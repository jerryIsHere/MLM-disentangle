{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install wikipedia --quiet\n!pip install transformers --quiet\n#!pip install bert-for-tf2 --quiet\n#!pip install tensorflow-datasets --quiet\n#!python -m spacy download xx_ent_wiki_sm --quiet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"languages = ['af',\n'sq',\n'am',\n'ar',\n'hy',\n'as',\n'az',\n'eu',\n'be',\n'bn',\n#'Bengali Romanize',\n'bs',\n'br',\n'bg',\n'my',\n#'Burmese zawgyi font',\n'ca',\n'zh',\n'zh-tw',\n'hr',\n'cs',\n'da',\n'nl',\n'en',\n'eo',\n'et',\n'tl',#'Filipino',\n'fi',\n'fr',\n'gl',\n'ka',\n'de',\n'el',\n'gu',\n'ha',\n'he',\n'hi',\n#'Hindi Romanize',\n'hu',\n'is',\n'id',\n'ga',\n'it',\n'ja',\n'jv',\n'kn',\n'kk',\n'km',\n'ko',\n'ku',\n#'Kyrgyz',\n'lo',\n'la',\n'lv',\n'lt',\n'mk',\n'mg',\n'ms',\n'ml',\n'mr',\n'mn',\n'ne',\n'no',\n'or',\n'om',\n'ps',\n'fa',\n'pl',\n'pt',\n'pa',\n'ro',\n'ru',\n'sa',\n'gd',\n'sr',\n'sd',\n#'Sinhala',\n'sk',\n'sl',\n'so',\n'es',\n'su',\n'sw',\n'sv',\n'ta',\n#'Tamil Romanize',\n'te',\n#'Telugu Romanize',\n'th',\n'tr',\n'uk',\n'ur',\n#'Urdu Romanize',\n'ug',\n'uz',\n'vi',\n'cy',\n'fy',\n'xh',\n'yi']\nimport wikipedia\nprint(str(len(languages)) + ' languages')\nfor lan in languages:\n    try:\n        wikipedia.set_lang(lan)\n    except:\n        print(lan +' not working')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"markdown","source":"\nwikipedia.set_lang('en') # tcy not in multilingual bert\npage = None\nwhile(page == None):\n    try:\n        page = wikipedia.page(title = wikipedia.random())\n    except:\n        pass\nline = [line for line in page.content.splitlines() if len(line.split()) > 0 and line.split()[0] !='==' and line.split()[0] !='===']\nfor l in line:\n    print(l)\n    #token = tokenizer.tokenize(l)\n    #print(token)\n    #print(tokenizer.convert_tokens_to_ids(token))","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from bert import bert_tokenization\nimport spacy\nimport tensorflow as tf\ntf.gfile = tf.io.gfile\ntokenizer = bert_tokenization.FullTokenizer(do_lower_case=False ,vocab_file = '/kaggle/input/bertbase-multilingual-cased-new-recommended/multi_cased_L-12_H-768_A-12/vocab.txt')\n\nnlp = spacy.load(\"xx_ent_wiki_sm\")\ndoc =  nlp('貓，通常指家貓，為小型貓科動物。根據遺傳學及考古學分析，人類養貓的紀錄可追溯至10,000年前的新月沃土地區，古埃及人飼養貓的紀錄可追溯至3,600年前，目的可能為捕鼠及其他齧齒目動物，以防止牠們吃掉榖物。')\nfor sent in doc.sents:\n    print(sent.text)\n    token = tokenizer.tokenize(sent.text)\n    print(token)\n    print(tokenizer.convert_tokens_to_ids(token))","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import wikipedia\nimport tensorflow as tf\nimport random\nfrom transformers import *\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\nclass WikiDataset(tf.data.Dataset):\n    def _generator():\n        for i in range(len(languages)):\n            wikipedia.set_lang(languages[i])\n            page = None\n            line = None\n            while(page == None):\n                try:\n                    page = wikipedia.page(title = wikipedia.random())\n                    line = [line for line in page.content.splitlines() if len(line.split()) > 0 and line.split()[0] !='==' and line.split()[0] !='===']\n                    if len(line) < 1:\n                        page = None\n                except:\n                    pass\n            \n            line = random.choice(line)\n            token = tokenizer.encode(line)\n            yield {'token': token , 'y_token': tf.one_hot(token,tokenizer.vocab_size),'y_language': tf.one_hot(i,len(languages))}\n    \n    def __new__(cls):\n        return tf.data.Dataset.from_generator(\n            cls._generator,\n            output_types={'token': tf.int32,\n                          'y_token': tf.float32,\n                          'y_language': tf.float32},\n            \n            output_shapes={'token':tf.TensorShape([ None]),\n                           'y_token': tf.TensorShape([None,tokenizer.vocab_size]),\n                           'y_language': tf.TensorShape([len(languages)])}\n\n        )\n    def padding(token):\n        WikiDataset.pad_id\n        \n        \ndef train_input_fn(epoch = 50):\n    def dataset_generator_fun(*args):\n        return WikiDataset()\n    \n    return (tf.data.Dataset.range(1)\n            .interleave(dataset_generator_fun\n                        , num_parallel_calls=tf.data.experimental.AUTOTUNE).repeat(epoch)#.shuffle(30)\n            .prefetch(tf.data.experimental.AUTOTUNE)\n           )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import *\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\nmodel = TFXLMRobertaModel.from_pretrained('xlm-roberta-large', from_pt=True)\nmodel.compile()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = train_input_fn().enumerate()\ni = 0\nfor line in ds:\n    data = line\n    result = model.predict(line[1]['token'])\n    i = i+1\n    if i > 0:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[1]['token'])\nprint(data[1]['y_token'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[0].shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}