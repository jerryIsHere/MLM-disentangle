{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Bert disentangle.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lJDkodJJsiMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wikipedia --quiet\n",
        "!pip install transformers --quiet\n",
        "#!pip install bert-for-tf2 --quiet\n",
        "#!pip install tensorflow-datasets --quiet\n",
        "#!python -m spacy download xx_ent_wiki_sm --quiet\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "mzgr1leYsiMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KSbYxCAfsiMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "languages = ['af',\n",
        "'sq',\n",
        "'am',\n",
        "'ar',\n",
        "'hy',\n",
        "'as',\n",
        "'az',\n",
        "'eu',\n",
        "'be',\n",
        "'bn',\n",
        "#'Bengali Romanize',\n",
        "'bs',\n",
        "'br',\n",
        "'bg',\n",
        "'my',\n",
        "#'Burmese zawgyi font',\n",
        "'ca',\n",
        "'zh',\n",
        "'zh-tw',\n",
        "'hr',\n",
        "'cs',\n",
        "'da',\n",
        "'nl',\n",
        "'en',\n",
        "'eo',\n",
        "'et',\n",
        "'tl',#'Filipino',\n",
        "'fi',\n",
        "'fr',\n",
        "'gl',\n",
        "'ka',\n",
        "'de',\n",
        "'el',\n",
        "'gu',\n",
        "'ha',\n",
        "'he',\n",
        "'hi',\n",
        "#'Hindi Romanize',\n",
        "'hu',\n",
        "'is',\n",
        "'id',\n",
        "'ga',\n",
        "'it',\n",
        "'ja',\n",
        "'jv',\n",
        "'kn',\n",
        "'kk',\n",
        "'km',\n",
        "'ko',\n",
        "'ku',\n",
        "#'Kyrgyz',\n",
        "'lo',\n",
        "'la',\n",
        "'lv',\n",
        "'lt',\n",
        "'mk',\n",
        "'mg',\n",
        "'ms',\n",
        "'ml',\n",
        "'mr',\n",
        "'mn',\n",
        "'ne',\n",
        "'no',\n",
        "'or',\n",
        "'om',\n",
        "'ps',\n",
        "'fa',\n",
        "'pl',\n",
        "'pt',\n",
        "'pa',\n",
        "'ro',\n",
        "'ru',\n",
        "'sa',\n",
        "'gd',\n",
        "'sr',\n",
        "'sd',\n",
        "#'Sinhala',\n",
        "'sk',\n",
        "'sl',\n",
        "'so',\n",
        "'es',\n",
        "'su',\n",
        "'sw',\n",
        "'sv',\n",
        "'ta',\n",
        "#'Tamil Romanize',\n",
        "'te',\n",
        "#'Telugu Romanize',\n",
        "'th',\n",
        "'tr',\n",
        "'uk',\n",
        "'ur',\n",
        "#'Urdu Romanize',\n",
        "'ug',\n",
        "'uz',\n",
        "'vi',\n",
        "'cy',\n",
        "'fy',\n",
        "'xh',\n",
        "'yi']\n",
        "import wikipedia\n",
        "print(str(len(languages)) + ' languages')\n",
        "for lan in languages:\n",
        "    try:\n",
        "        wikipedia.set_lang(lan)\n",
        "    except:\n",
        "        print(lan +' not working')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "xac0m2m3siMi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "wikipedia.set_lang('en') # tcy not in multilingual bert\n",
        "page = None\n",
        "while(page == None):\n",
        "    try:\n",
        "        page = wikipedia.page(title = wikipedia.random())\n",
        "    except:\n",
        "        pass\n",
        "line = [line for line in page.content.splitlines() if len(line.split()) > 0 and line.split()[0] !='==' and line.split()[0] !='===']\n",
        "for l in line:\n",
        "    print(l)\n",
        "    #token = tokenizer.tokenize(l)\n",
        "    #print(token)\n",
        "    #print(tokenizer.convert_tokens_to_ids(token))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "S7HmrW4asiMj",
        "colab_type": "text"
      },
      "source": [
        "from bert import bert_tokenization\n",
        "import spacy\n",
        "import tensorflow as tf\n",
        "tf.gfile = tf.io.gfile\n",
        "tokenizer = bert_tokenization.FullTokenizer(do_lower_case=False ,vocab_file = '/kaggle/input/bertbase-multilingual-cased-new-recommended/multi_cased_L-12_H-768_A-12/vocab.txt')\n",
        "\n",
        "nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
        "doc =  nlp('貓，通常指家貓，為小型貓科動物。根據遺傳學及考古學分析，人類養貓的紀錄可追溯至10,000年前的新月沃土地區，古埃及人飼養貓的紀錄可追溯至3,600年前，目的可能為捕鼠及其他齧齒目動物，以防止牠們吃掉榖物。')\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)\n",
        "    token = tokenizer.tokenize(sent.text)\n",
        "    print(token)\n",
        "    print(tokenizer.convert_tokens_to_ids(token))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gF0qUUEssiMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wikipedia\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from transformers import *\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "class WikiDataset(tf.data.Dataset):\n",
        "    def _generator():\n",
        "        for i in range(len(languages)):\n",
        "            wikipedia.set_lang(languages[i])\n",
        "            page = None\n",
        "            line = None\n",
        "            while(page == None):\n",
        "                try:\n",
        "                    page = wikipedia.page(title = wikipedia.random())\n",
        "                    line = [line for line in page.content.splitlines() if len(line.split()) > 0 and line.split()[0] !='==' and line.split()[0] !='===']\n",
        "                    if len(line) < 1:\n",
        "                        page = None\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            line = random.choice(line)\n",
        "            token = tokenizer.encode(line)\n",
        "            yield {'token': token , 'y_token': tf.one_hot(token,tokenizer.vocab_size),'y_language': tf.one_hot(i,len(languages))}\n",
        "    \n",
        "    def __new__(cls):\n",
        "        return tf.data.Dataset.from_generator(\n",
        "            cls._generator,\n",
        "            output_types={'token': tf.int32,\n",
        "                          'y_token': tf.float32,\n",
        "                          'y_language': tf.float32},\n",
        "            \n",
        "            output_shapes={'token':tf.TensorShape([ None]),\n",
        "                           'y_token': tf.TensorShape([None,tokenizer.vocab_size]),\n",
        "                           'y_language': tf.TensorShape([len(languages)])}\n",
        "\n",
        "        )\n",
        "    def padding(token):\n",
        "        WikiDataset.pad_id\n",
        "        \n",
        "        \n",
        "def train_input_fn(epoch = 50):\n",
        "    def dataset_generator_fun(*args):\n",
        "        return WikiDataset()\n",
        "    \n",
        "    return (tf.data.Dataset.range(1)\n",
        "            .interleave(dataset_generator_fun\n",
        "                        , num_parallel_calls=tf.data.experimental.AUTOTUNE).repeat(epoch)#.shuffle(30)\n",
        "            .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "           )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "q1GWUcR8siMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from transformers import *\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n",
        "model = TFXLMRobertaModel.from_pretrained('xlm-roberta-large', from_pt=True)\n",
        "model.compile()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IkyWnw42siMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = train_input_fn().enumerate()\n",
        "i = 0\n",
        "for line in ds:\n",
        "    data = line\n",
        "    result = model.predict(line[1]['token'])\n",
        "    i = i+1\n",
        "    if i > 0:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PgcOc6MUsiMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(data[1]['token'])\n",
        "print(data[1]['y_token'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2pEjS3jUsiMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}